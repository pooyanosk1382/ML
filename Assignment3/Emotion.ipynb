{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:30.615149900Z",
     "start_time": "2024-06-04T10:36:30.614143800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before anything we add a name for each column to make them meaningful and easy to use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Sentence Emotion\n0     کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان     SAD\n1     عکسی که چند روز پیش گذاشته بودم این فیلم الانش...   OTHER\n2     تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که ه...     SAD\n3              خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه   HAPPY\n4     این خاک مال مردمان است نه حاکمان #ایران #مهسا_...   ANGRY\n...                                                 ...     ...\n4919  من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه   HAPPY\n4920  گاز نداریم آب نداریم برق نداریم نت نداریم پول ...     SAD\n4921  یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عا...     SAD\n4922  زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوا...   OTHER\n4923  سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگی...     SAD\n\n[4924 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان</td>\n      <td>SAD</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>عکسی که چند روز پیش گذاشته بودم این فیلم الانش...</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که ه...</td>\n      <td>SAD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه</td>\n      <td>HAPPY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>این خاک مال مردمان است نه حاکمان #ایران #مهسا_...</td>\n      <td>ANGRY</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4919</th>\n      <td>من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه</td>\n      <td>HAPPY</td>\n    </tr>\n    <tr>\n      <th>4920</th>\n      <td>گاز نداریم آب نداریم برق نداریم نت نداریم پول ...</td>\n      <td>SAD</td>\n    </tr>\n    <tr>\n      <th>4921</th>\n      <td>یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عا...</td>\n      <td>SAD</td>\n    </tr>\n    <tr>\n      <th>4922</th>\n      <td>زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوا...</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>4923</th>\n      <td>سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگی...</td>\n      <td>SAD</td>\n    </tr>\n  </tbody>\n</table>\n<p>4924 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_data.csv')\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:30.762661500Z",
     "start_time": "2024-06-04T10:36:30.615149900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(4924, 2)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:30.770532Z",
     "start_time": "2024-06-04T10:36:30.762661500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Sentence    0\nEmotion     0\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:30.782697100Z",
     "start_time": "2024-06-04T10:36:30.770532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: >"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = data.Emotion.value_counts()\n",
    "plot_df.plot(kind=\"bar\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:31.023346800Z",
     "start_time": "2024-06-04T10:36:30.782697100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "{'♥', '’', '[', 'ش', '_', '“', '٦', '⚽', 'Y', 'آ', '*', '3', 'y', 'ة', '°', '=', 'ق', '.', 'I', 'K', 'ۀ', 'z', 'c', 'H', 'R', '✋', '۹', 'ژ', '۲', 'ج', 'S', 'ِ', '/', 'ʏ', '^', 'گ', 'P', 'Q', '✨', 'g', '&', 'ᴏ', 'چ', 'س', '۱', '0', 'ـ', 'A', '♂', 'و', 'غ', '️', ']', '☠', 'د', 'w', 'h', 'F', '❤', 'ؤ', ' ', 'u', 'k', 'n', '#', '‐', '”', '8', 'ر', 'ئ', '9', 'd', '?', 'ف', '؟', '✍', 'p', 'V', 'ا', '☄', '۳', 'ع', 'خ', '6', '۰', 't', '\\n', 'م', 'ى', 'E', '⃟', 'ي', '۸', 'ّ', 'x', '☺', '4', 'O', '⁉', 'ز', '⭕', '!', '☘', '⭐', 'ی', 'ط', 'N', '⚘', 'ُ', '۴', 'j', 'D', '+', 'Z', 'َ', 'ᴛ', ',', 'B', 'ً', 'a', 'l', 'ᴇ', 'e', 'M', 'U', '…', 'v', 'm', '|', '\\u200c', '⛓', '¹', 'ه', '؛', '%', '•', '\\u2066', '١', 'T', '٣', '♀', '⊰', 'b', '(', '✌', 'ْ', 'ټ', '‘', '۶', '7', ')', ';', 'ہ', '\\u200d', '5', 'ت', 'ە', '٪', 'J', 'ء', ':', 'ص', 'ل', '۷', '\\u2067', '٠', '٫', '٢', '✅', '❄', '2', '»', 'o', '}', 'ھ', 'L', 'f', 'ٔ', '1', 'W', '☹', 'پ', '☝', 'i', 'G', 'ح', '۔', '،', 'r', '۵', 'ے', '~', '\\u2069', 'ك', 's', 'ب', '-', 'ض', 'ک', 'ن', '٬', 'ذ', '²', 'ɴ', 'q', 'C', 'ث', 'أ', '«', 'ظ'}\n"
     ]
    }
   ],
   "source": [
    "uniqueChars = set(''.join(data['Sentence']))\n",
    "print(len(uniqueChars))\n",
    "print(uniqueChars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:31.023346800Z",
     "start_time": "2024-06-04T10:36:30.993512600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see there are capital and small letter. Also, there are three kind of 'ی' for only one char and there are persian and english numbers. We have to handle all of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "{'♥', '’', '[', 'ش', '_', '“', '٦', '⚽', 'آ', '*', '3', 'y', 'ة', '°', '=', 'ق', '.', 'ۀ', 'z', 'c', '✋', '۹', 'ژ', '۲', 'ج', 'ِ', '/', 'ʏ', '^', 'گ', '✨', 'g', '&', 'ᴏ', 'چ', 'س', '۱', '0', 'ـ', '♂', 'و', 'غ', '️', ']', '☠', 'د', 'w', 'h', '❤', 'ؤ', ' ', 'u', 'k', 'n', '#', '‐', '”', '8', 'ر', 'ئ', '9', 'd', '?', 'ف', '؟', '✍', 'p', '☄', 'ا', '۳', 'ع', 'خ', '6', '۰', 't', '\\n', 'م', 'ى', '⃟', 'ي', '۸', 'ّ', 'x', '☺', '4', '⁉', 'ز', '⭕', '!', '☘', '⭐', 'ی', 'ط', '⚘', 'ُ', '۴', 'j', '+', 'َ', 'ᴛ', ',', 'ً', 'a', 'l', 'ᴇ', 'e', '|', '…', 'v', 'm', '¹', '\\u200c', '⛓', 'ه', '؛', '%', '•', '\\u2066', '١', '٣', '♀', '⊰', 'b', '(', '✌', 'ْ', 'ټ', '‘', '۶', '7', ')', ';', 'ہ', '\\u200d', '5', 'ت', 'ە', '٪', 'ء', ':', 'ص', 'ل', '۷', '\\u2067', '٠', '٫', '٢', '✅', '❄', '2', '»', 'o', '}', 'ھ', 'f', 'ٔ', '1', '☹', 'پ', '☝', 'i', 'ح', '۔', '،', 'r', '۵', 'ے', '~', '\\u2069', 'ك', 's', 'ب', '-', 'ض', 'ک', 'ن', '٬', 'ذ', '²', 'ɴ', 'q', 'ث', 'أ', '«', 'ظ'}\n"
     ]
    }
   ],
   "source": [
    "data['Sentence'] = data['Sentence'].str.lower()\n",
    "uniqueChars = set(''.join(data['Sentence']))\n",
    "print(len(uniqueChars))\n",
    "print(uniqueChars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:31.044008100Z",
     "start_time": "2024-06-04T10:36:31.015839600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "{'♥', '’', '[', 'ش', '_', '“', '⚽', 'آ', '*', '3', 'y', 'ة', '°', '=', 'ق', '.', 'ۀ', 'z', 'c', '✋', 'ژ', 'ج', 'ِ', '/', 'ʏ', '^', 'گ', '✨', 'g', '&', 'ᴏ', 'چ', 'س', '0', 'ـ', '♂', 'و', 'غ', '️', ']', '☠', 'د', 'w', 'h', '❤', 'ؤ', ' ', 'u', 'k', 'n', '#', '‐', '8', '”', 'ر', 'ئ', '9', 'd', '?', 'ف', '؟', '✍', 'p', '☄', 'ا', 'ع', 'خ', '6', 't', '\\n', 'م', 'ى', '⃟', 'ي', 'x', 'ّ', '⁉', '☺', '4', 'ز', '⭕', '!', '☘', '⭐', 'ی', 'ط', '⚘', 'ُ', 'j', '+', 'َ', 'ᴛ', ',', 'ً', 'a', 'l', 'ᴇ', 'e', '|', '…', 'v', 'm', '¹', '\\u200c', '⛓', 'ه', '؛', '%', '•', '\\u2066', '١', '٣', '♀', '⊰', 'b', '(', '✌', 'ْ', 'ټ', '‘', '7', ')', ';', 'ہ', '\\u200d', '5', 'ت', 'ە', '٪', 'ء', ':', 'ص', 'ل', '\\u2067', '٠', '٫', '٢', '✅', '❄', '2', '»', 'o', '}', 'ھ', 'f', 'ٔ', '1', '☹', 'پ', '☝', 'i', 'ح', '۔', '،', 'r', 'ے', '~', '\\u2069', 'ك', 's', 'ب', '-', 'ض', 'ک', 'ن', '٬', 'ذ', '²', 'ɴ', 'q', 'ث', 'أ', '«', 'ظ'}\n"
     ]
    }
   ],
   "source": [
    "def convert_persian_to_english(text):\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    translation_table = str.maketrans(persian_digits, english_digits)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "data['Sentence'] = data['Sentence'].apply(convert_persian_to_english)\n",
    "uniqueChars = set(''.join(data['Sentence']))\n",
    "print(len(uniqueChars))\n",
    "print(uniqueChars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:31.211918300Z",
     "start_time": "2024-06-04T10:36:31.044008100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we handle the numbers and upper cases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "{'♥', '’', '[', 'ش', '_', '“', '⚽', 'آ', '*', '3', 'y', 'ة', '°', '=', 'ق', '.', 'ۀ', 'z', 'c', '✋', 'ژ', 'ج', 'ِ', '/', 'ʏ', '^', 'گ', '✨', 'g', '&', 'ᴏ', 'چ', 'س', '0', 'ـ', '♂', 'و', 'غ', '️', ']', '☠', 'د', 'w', 'h', '❤', 'ؤ', ' ', 'u', 'k', 'n', '#', '‐', '8', '”', 'ر', '9', 'd', '?', 'ف', '؟', '✍', 'p', '☄', 'ا', 'ع', 'خ', '6', 't', '\\n', 'م', '⃟', 'x', '⁉', 'ّ', '☺', '4', 'ز', '⭕', '!', '☘', '⭐', 'ی', 'ط', '⚘', 'ُ', 'j', '+', 'َ', 'ᴛ', ',', 'ً', 'a', 'l', 'ᴇ', 'e', '|', '…', 'v', 'm', '¹', '\\u200c', '⛓', 'ه', '؛', '%', '•', '\\u2066', '١', '٣', '♀', '⊰', 'b', '(', '✌', 'ْ', 'ټ', '‘', '7', ')', ';', 'ہ', '\\u200d', '5', 'ت', 'ە', '٪', 'ء', ':', 'ص', 'ل', '\\u2067', '٠', '٫', '٢', '✅', '❄', '2', '»', 'o', '}', 'ھ', 'f', 'ٔ', '1', '☹', 'پ', '☝', 'i', 'ح', '۔', '،', 'r', '~', '\\u2069', 'ك', 's', 'ب', '-', 'ض', 'ک', 'ن', '٬', 'ذ', '²', 'ɴ', 'q', 'ث', 'أ', '«', 'ظ'}\n"
     ]
    }
   ],
   "source": [
    "def normalize_yeh(text):\n",
    "    # The three types of \"ی\" in Persian: ی, ي, ے, ئ\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "    # Convert all types of \"ی\" to the standard \"ی\"\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "    return text\n",
    "\n",
    "# Apply the normalization function to the 'text' column\n",
    "data['Sentence'] = data['Sentence'].apply(normalize_yeh)\n",
    "uniqueChars = set(''.join(data['Sentence']))\n",
    "print(len(uniqueChars))\n",
    "print(uniqueChars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:31.211918300Z",
     "start_time": "2024-06-04T10:36:31.121252800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "F1-score: 0.61\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:32.386319200Z",
     "start_time": "2024-06-04T10:36:31.121252800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n",
      "F1-score: 0.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:43.946168400Z",
     "start_time": "2024-06-04T10:36:32.386319200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "F1-score: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = SVC(kernel = 'rbf', random_state = 0)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:46.832204200Z",
     "start_time": "2024-06-04T10:36:43.946168400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "F1-score: 0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:47.144954900Z",
     "start_time": "2024-06-04T10:36:46.838710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "F1-score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:36:50.051597900Z",
     "start_time": "2024-06-04T10:36:47.144954900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By cross validation approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.59\n",
      "Average F1-score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Train the XGBoost model using cross-validation\n",
    "model = XGBClassifier()\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_tfidf, df['emotion']):\n",
    "    X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = df['emotion'].iloc[train_index], df['emotion'].iloc[test_index]\n",
    "\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Average Accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}')\n",
    "print(f'Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:44:40.848154700Z",
     "start_time": "2024-06-04T10:44:33.070667200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.60\n",
      "Average F1-score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Train the XGBoost model using cross-validation\n",
    "model = SVC(kernel = 'rbf', random_state = 0)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_tfidf, df['emotion']):\n",
    "    X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = df['emotion'].iloc[train_index], df['emotion'].iloc[test_index]\n",
    "\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Average Accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}')\n",
    "print(f'Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:45:56.799201500Z",
     "start_time": "2024-06-04T10:45:43.121743400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.54\n",
      "Average F1-score: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Train the XGBoost model using cross-validation\n",
    "model = KNeighborsClassifier()\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_tfidf, df['emotion']):\n",
    "    X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = df['emotion'].iloc[train_index], df['emotion'].iloc[test_index]\n",
    "\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Average Accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}')\n",
    "print(f'Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:47:52.498302800Z",
     "start_time": "2024-06-04T10:47:51.251713300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.58\n",
      "Average F1-score: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Train the XGBoost model using cross-validation\n",
    "model = RandomForestClassifier()\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_tfidf, df['emotion']):\n",
    "    X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = df['emotion'].iloc[train_index], df['emotion'].iloc[test_index]\n",
    "\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Average Accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}')\n",
    "print(f'Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:51:45.695916100Z",
     "start_time": "2024-06-04T10:50:30.780432300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.61\n",
      "Average F1-score: 0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Train the XGBoost model using cross-validation\n",
    "model = LogisticRegression()\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_tfidf, df['emotion']):\n",
    "    X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = df['emotion'].iloc[train_index], df['emotion'].iloc[test_index]\n",
    "\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Average Accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}')\n",
    "print(f'Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T10:51:54.547952900Z",
     "start_time": "2024-06-04T10:51:50.893968300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Average Accuracy: 0.53\n",
      "Decision Tree - Average F1-score: 0.53\n",
      "Random Forest - Average Accuracy: 0.59\n",
      "Random Forest - Average F1-score: 0.58\n",
      "Gradient Boosting - Average Accuracy: 0.58\n",
      "Gradient Boosting - Average F1-score: 0.58\n",
      "XGBoost - Average Accuracy: 0.59\n",
      "XGBoost - Average F1-score: 0.59\n",
      "AdaBoost - Average Accuracy: 0.47\n",
      "AdaBoost - Average F1-score: 0.45\n",
      "Extra Tree - Average Accuracy: 0.61\n",
      "Extra Tree - Average F1-score: 0.60\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7474\n",
      "[LightGBM] [Info] Number of data points in the train set: 3939, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -1.582883\n",
      "[LightGBM] [Info] Start training from score -2.665554\n",
      "[LightGBM] [Info] Start training from score -1.213923\n",
      "[LightGBM] [Info] Start training from score -1.360977\n",
      "[LightGBM] [Info] Start training from score -1.762489\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7514\n",
      "[LightGBM] [Info] Number of data points in the train set: 3939, number of used features: 318\n",
      "[LightGBM] [Info] Start training from score -1.582883\n",
      "[LightGBM] [Info] Start training from score -2.665554\n",
      "[LightGBM] [Info] Start training from score -1.213923\n",
      "[LightGBM] [Info] Start training from score -1.360977\n",
      "[LightGBM] [Info] Start training from score -1.762489\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7439\n",
      "[LightGBM] [Info] Number of data points in the train set: 3939, number of used features: 315\n",
      "[LightGBM] [Info] Start training from score -1.581648\n",
      "[LightGBM] [Info] Start training from score -2.669210\n",
      "[LightGBM] [Info] Start training from score -1.213923\n",
      "[LightGBM] [Info] Start training from score -1.360977\n",
      "[LightGBM] [Info] Start training from score -1.762489\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7437\n",
      "[LightGBM] [Info] Number of data points in the train set: 3939, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -1.581648\n",
      "[LightGBM] [Info] Start training from score -2.669210\n",
      "[LightGBM] [Info] Start training from score -1.214778\n",
      "[LightGBM] [Info] Start training from score -1.359987\n",
      "[LightGBM] [Info] Start training from score -1.762489\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7410\n",
      "[LightGBM] [Info] Number of data points in the train set: 3940, number of used features: 309\n",
      "[LightGBM] [Info] Start training from score -1.581902\n",
      "[LightGBM] [Info] Start training from score -2.665808\n",
      "[LightGBM] [Info] Start training from score -1.215032\n",
      "[LightGBM] [Info] Start training from score -1.360241\n",
      "[LightGBM] [Info] Start training from score -1.762743\n",
      "LightGBM - Average Accuracy: 0.56\n",
      "LightGBM - Average F1-score: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Train the tree-based models using cross-validation\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(criterion='gini'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Extra Tree': ExtraTreesClassifier(),\n",
    "    'LightGBM': LGBMClassifier()\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X_tfidf, df['emotion']):\n",
    "        X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]\n",
    "        y_train, y_test = df['emotion'].iloc[train_index], df['emotion'].iloc[test_index]\n",
    "\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    print(f'{model_name} - Average Accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}')\n",
    "    print(f'{model_name} - Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T11:26:49.853200400Z",
     "start_time": "2024-06-04T11:19:59.080484500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Decision Tree model...\n",
      "Decision Tree - Best Hyperparameters: {'criterion': 'gini', 'max_depth': None}\n",
      "Optimizing Random Forest model...\n",
      "Random Forest - Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Optimizing Gradient Boosting model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [25], line 72\u001B[0m\n\u001B[0;32m     70\u001B[0m model \u001B[38;5;241m=\u001B[39m model_class()\n\u001B[0;32m     71\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(model, param_grid\u001B[38;5;241m=\u001B[39mparam_grids[model_name], cv\u001B[38;5;241m=\u001B[39mStratifiedKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m), scoring\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1_weighted\u001B[39m\u001B[38;5;124m'\u001B[39m], refit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 72\u001B[0m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_tfidf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43memotion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m#print(f'{model_name} - Best Accuracy: {grid_search.best_score_[\"accuracy\"]:.2f}')\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m#print(f'{model_name} - Best F1-score: {grid_search.best_score_[\"f1_weighted\"]:.2f}')\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Best Hyperparameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid_search\u001B[38;5;241m.\u001B[39mbest_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    868\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    869\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    870\u001B[0m     )\n\u001B[0;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 874\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    878\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1387\u001B[0m     \u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1388\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    814\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    816\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    817\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    818\u001B[0m         )\n\u001B[0;32m    819\u001B[0m     )\n\u001B[1;32m--> 821\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    823\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    828\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    829\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    831\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    833\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    834\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    835\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    836\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    839\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    841\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    842\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    843\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1088\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1085\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1088\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1089\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1092\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    684\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    685\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 686\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    689\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    690\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001B[0m, in \u001B[0;36mBaseGradientBoosting.fit\u001B[1;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[0;32m    535\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resize_state()\n\u001B[0;32m    537\u001B[0m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m n_stages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_stages \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stages\u001B[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[0m\n\u001B[0;32m    608\u001B[0m     old_oob_score \u001B[38;5;241m=\u001B[39m loss_(\n\u001B[0;32m    609\u001B[0m         y[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[0;32m    610\u001B[0m         raw_predictions[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[0;32m    611\u001B[0m         sample_weight[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[0;32m    612\u001B[0m     )\n\u001B[0;32m    614\u001B[0m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[1;32m--> 615\u001B[0m raw_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    616\u001B[0m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    619\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    620\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    624\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;66;03m# track deviance (= loss)\u001B[39;00m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:257\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stage\u001B[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[0m\n\u001B[0;32m    254\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;241m*\u001B[39m sample_mask\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[0;32m    256\u001B[0m X \u001B[38;5;241m=\u001B[39m X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[1;32m--> 257\u001B[0m \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresidual\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[0;32m    260\u001B[0m loss\u001B[38;5;241m.\u001B[39mupdate_terminal_regions(\n\u001B[0;32m    261\u001B[0m     tree\u001B[38;5;241m.\u001B[39mtree_,\n\u001B[0;32m    262\u001B[0m     X,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    269\u001B[0m     k\u001B[38;5;241m=\u001B[39mk,\n\u001B[0;32m    270\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1219\u001B[0m     \u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m   1220\u001B[0m \n\u001B[0;32m   1221\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1244\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1247\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1249\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1250\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1252\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    370\u001B[0m         splitter,\n\u001B[0;32m    371\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    377\u001B[0m     )\n\u001B[1;32m--> 379\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalize Persian characters\n",
    "    text = normalize_persian(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Add more preprocessing steps as needed\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_persian(text):\n",
    "    # Define a function to normalize Persian characters\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹٦'\n",
    "    english_digits = '01234567896'\n",
    "    yeh_characters = ['ی', 'ي', 'ے', 'ئ', 'ى']\n",
    "\n",
    "    text = text.translate(str.maketrans(persian_digits, english_digits))\n",
    "    for char in yeh_characters:\n",
    "        text = text.replace(char, 'ی')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "df['Sentence'] = df['Sentence'].apply(preprocess_text)\n",
    "encoder = OrdinalEncoder()\n",
    "# Fit and transform the 'emotion' column\n",
    "df['emotion'] = encoder.fit_transform(df[['Emotion']])\n",
    "\n",
    "# Extract features using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Sentence'])\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Decision Tree': {'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, None]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 150], 'max_depth': [5, 10, 15, None]},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.2, 0.3]},\n",
    "    'XGBoost': {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.2, 0.3]},\n",
    "    'AdaBoost': {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.5, 1.0]},\n",
    "    'Extra Tree': {'n_estimators': [50, 100, 150], 'criterion': ['gini', 'entropy']},\n",
    "    'LightGBM': {'num_leaves': [31, 63, 127], 'learning_rate': [0.1, 0.2, 0.3]}\n",
    "}\n",
    "\n",
    "# Train the tree-based models using cross-validation with hyperparameter optimization\n",
    "for model_name, model_class in [\n",
    "    ('Decision Tree', DecisionTreeClassifier),\n",
    "    ('Random Forest', RandomForestClassifier),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier),\n",
    "    ('XGBoost', XGBClassifier),\n",
    "    ('AdaBoost', AdaBoostClassifier),\n",
    "    ('Extra Tree', ExtraTreesClassifier),\n",
    "    ('LightGBM', LGBMClassifier)\n",
    "]:\n",
    "    print(f'Optimizing {model_name} model...')\n",
    "    model = model_class()\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grids[model_name], cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring=['accuracy', 'f1_weighted'], refit='accuracy')\n",
    "    grid_search.fit(X_tfidf, df['emotion'])\n",
    "\n",
    "    #print(f'{model_name} - Best Accuracy: {grid_search.best_score_[\"accuracy\"]:.2f}')\n",
    "    #print(f'{model_name} - Best F1-score: {grid_search.best_score_[\"f1_weighted\"]:.2f}')\n",
    "    print(f'{model_name} - Best Hyperparameters: {grid_search.best_params_}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T12:05:51.878677800Z",
     "start_time": "2024-06-04T11:42:51.469754Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
